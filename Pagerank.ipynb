{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9122515d",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "https://www.kaggle.com/code/ldegioanni/covid-19-papers-ranking/notebook#Part-I:-Data-Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df28177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Sandbox\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#To generate a refid for each paper (dataset + bib_entries)\n",
    "import hashlib #for sha1\n",
    "\n",
    "#To build network and compute pagerank\n",
    "import networkx as nx\n",
    "import math as math\n",
    "\n",
    "#For Data viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048b7e6",
   "metadata": {},
   "source": [
    "# Constants\n",
    "\n",
    "$P^{2}_{author}=(0.25)*P_{coauthornetwork}(author)+(0.75)*\\sum^{publication}{P_{citationnetwork}(pubilication)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12800fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight parameters for Approach 2 and 3 :\n",
    "\n",
    "weights_InfluenceScore = [0.25, 0.25, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b46b61",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "\n",
    "- os.walk()： **for directory tree** is a Python method that generates the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02f7b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files Loaded:  85371\n"
     ]
    }
   ],
   "source": [
    "#1. Get the data\n",
    "datafiles = []\n",
    "for dirname, _, filenames in os.walk('/Users/Shared/Files From e.localized/Singapore/Semester2/PC5253 Complex system modeling/PageRank/Kaggle/input'):\n",
    "    \n",
    "    for filename in filenames:\n",
    "            ifile = os.path.join(dirname, filename)\n",
    "            # datafiles: 存储json文件\n",
    "            if ifile.split(\".\")[-1] == \"json\":  \n",
    "                datafiles.append(ifile)\n",
    "            \n",
    "print(\"Number of Files Loaded: \", len(datafiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4c229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading metadata csv file to get the publish time\n",
    "metadata = pd.read_csv(\"/Users/Shared/Files From e.localized/Singapore/Semester2/PC5253 Complex system modeling/PageRank//Kaggle/input/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6f28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Creating of the two DataFrames:\n",
    "#dfPaperList = df of Research Papers.. Variables: paper_id, paper_title, paper_authors\n",
    "#dfCitationsFlat = df of all the citations . Variables: citationsId, paperId (where the citation is made),refid, title, year\n",
    "\n",
    "authors = [] # paper id+ author name\n",
    "\n",
    "citationsFlat = []\n",
    "citationsCount = 0\n",
    "\n",
    "for file in datafiles:\n",
    "    with open(file,'r')as f:#opening a file in read mode\n",
    "        doc = json.load(f)#doc is a dictionary, json.loas()return dictionary object\n",
    "    paper_id = doc['paper_id']\n",
    "    \n",
    "    paper_authors = []\n",
    "\n",
    "    for value in doc['metadata']['authors']:\n",
    "        if len(doc['metadata']['authors']) == 0:\n",
    "            paper_authors.append(\"NA\")\n",
    "        else:\n",
    "            last = value[\"last\"]\n",
    "            first = value[\"first\"]#first name ; last name\n",
    "            paper_authors.append(first+\" \"+last)\n",
    "\n",
    "    authors.append({\"paper_id\": paper_id, \"authors\" : paper_authors})\n",
    "\n",
    "    for key,value in doc['bib_entries'].items():\n",
    "        refid = key\n",
    "        title = value['title'].lower()\n",
    "        year = value['year']\n",
    "        venue = value['venue'] \n",
    "        SHATitleCitation = hashlib.sha1(title.lower().encode()).hexdigest() #\n",
    "\n",
    "        if (len(title) == 0):\n",
    "            continue #there is noting we can do without any title\n",
    "\n",
    "        citationsFlat.append({\"citationId\":citationsCount,\\\n",
    "                          \"refid\" : SHATitleCitation,\\\n",
    "                          \"from\": paper_id,\\\n",
    "                          \"title\": title.lower(),\\\n",
    "                          \"year\": year})\n",
    "        citationsCount=citationsCount+1\n",
    "        \n",
    "#Conversion into DataFrame\n",
    "dfCitationsFlat = pd.DataFrame(citationsFlat)# citationsId, paperId (where the citation is made),refid, title, year\n",
    "authorsDf = pd.DataFrame(authors) # paper id+ author name\n",
    "\n",
    "metadata_extract = metadata[[\"sha\", \"title\", \"abstract\", \"publish_time\"]].rename(columns = {\"sha\" : \"paper_id\"})\n",
    "dfPaperList = pd.merge(metadata_extract, authorsDf, on = \"paper_id\", how = \"left\")\n",
    "# on the common column \"paper_id\" and use left join\n",
    "\n",
    "dfPaperList[\"year\"] = 0\n",
    "dfPaperList[\"refid\"] = \"\"\n",
    "\n",
    "for i in range(len(dfPaperList)):\n",
    "    \n",
    "    dfPaperList[\"refid\"][i] =  hashlib.sha1(str(dfPaperList[\"title\"][i]).lower().encode()).hexdigest()\n",
    "     #NB: We are building a custom identifier based on papers titles to ensure identification will be consistent between the papers in the Research Dataset and the papers extracted from the bib entries.\n",
    "     #Unfortunately a paperId is not present for citations and doi is not provided for the whole dataset but title seem to be present for ~98% of the dataset. To enable and ease indexing capabilities we are hashing with SHA   \n",
    "    dfPaperList[\"year\"][i] = str(dfPaperList[\"publish_time\"][i])[:4] # just want to get year\n",
    "    \n",
    "    try:\n",
    "        dfPaperList[\"authors\"][i] = dfPaperList[\"authors\"][i].split(\";\")\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "quotationPapersFreq = pd.DataFrame({\"refid\" : dfCitationsFlat[\"refid\"].value_counts().index, \n",
    "                       \"nbQuotations\" : dfCitationsFlat[\"title\"].value_counts().values}) \n",
    "\n",
    "paperToScore = pd.merge(dfPaperList,quotationPapersFreq, on = \"refid\", how = \"left\")\n",
    "paperToScore[\"nbQuotations\"] = paperToScore[\"nbQuotations\"].fillna(0)\n",
    "#This is a Python code snippet that fills the missing values in the nbQuotations column of the paperToScore dataframe with 0.\n",
    "\n",
    "\n",
    "#Adding list of references by papers according to the refid\n",
    "refList = pd.DataFrame({\"references\" : dfCitationsFlat.groupby('from')['refid'].apply(list)}) \n",
    "refList[\"paper_id\"] = refList.index; cols = [\"paper_id\",\"references\"] ; refList = refList[cols].reset_index(drop = True) #Reformatting the reflist by papers\n",
    "datasetForScoring = pd.merge(paperToScore, refList, how='left', on = 'paper_id').reset_index(drop = True)\n",
    "#This code is grouping the refid column of the dfCitationsFlat dataframe by the values in the from column and applying the list function to the resulting groups. The list function creates a list of all the refid values in each group.\n",
    "\n",
    "datasetForScoring = datasetForScoring[(datasetForScoring[\"authors\"].isna() == False)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5fb3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Papers in the CORD-19 dataset : 63571\n",
      "Number of Citations found in the CORD-19 dataset : 4208974\n",
      "Citations with no title:  0\n",
      "Number of duplicated research paper titles:  1421\n",
      "Number of duplicated citations titles:  2543820\n",
      "Number of Papers that will be scored:  46407\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>refid</th>\n",
       "      <th>nbQuotations</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b2897e1277f56641193a6db73825f707eed3e4c9</td>\n",
       "      <td>Sequence requirements for RNA strand transfer ...</td>\n",
       "      <td>Nidovirus subgenomic mRNAs contain a leader se...</td>\n",
       "      <td>2001-12-17</td>\n",
       "      <td>[Alexander Pasternak, Erwin Van Den Born, Will...</td>\n",
       "      <td>2001</td>\n",
       "      <td>7890bdcde2bc48da8b35296ee38c3aa6e6a549c5</td>\n",
       "      <td>79.0</td>\n",
       "      <td>[63f53cf95376af6f781ae6c60df4887012432de5, 02c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e3d0d482ebd9a8ba81c254cc433f314142e72174</td>\n",
       "      <td>Crystal structure of murine sCEACAM1a[1,4]: a ...</td>\n",
       "      <td>CEACAM1 is a member of the carcinoembryonic an...</td>\n",
       "      <td>2002-05-01</td>\n",
       "      <td>[Kemin Tan, Bruce Zelus, Rob Meijers, Jin-Huan...</td>\n",
       "      <td>2002</td>\n",
       "      <td>9555d93a1e7d86c279a9cb7f40e1935ac998cb00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>[10e50d9f52a7b77e7d89559892a5898a5a202feb, 736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00b1d99e70f779eb4ede50059db469c65e8c1469</td>\n",
       "      <td>Synthesis of a novel hepatitis C virus protein...</td>\n",
       "      <td>Hepatitis C virus (HCV) is an important human ...</td>\n",
       "      <td>2001-07-16</td>\n",
       "      <td>[Zhenming Xu, Jinah Choi, T Yen, Wen Lu, Anne ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>592f6ad0e68ffa8c56a5cd5d2eb673bf092b02b3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[11e6080d95677abd1aa156897750ce06c626df43, c35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0</td>\n",
       "      <td>Structure of coronavirus main proteinase revea...</td>\n",
       "      <td>The key enzyme in coronavirus polyprotein proc...</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>[Kanchan Anand, Gottfried Palm, Jeroen Mesters...</td>\n",
       "      <td>2002</td>\n",
       "      <td>6df07660b30c131c1d75c1a5974b75faada1044f</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[65886aa952fa05a113ca27ed8334a3adad3a4aa0, a9a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dde02f11923815e6a16a31dd6298c46b109c5dfa</td>\n",
       "      <td>Discontinuous and non-discontinuous subgenomic...</td>\n",
       "      <td>Arteri-, corona-, toro- and roniviruses are ev...</td>\n",
       "      <td>2002-12-01</td>\n",
       "      <td>[A Van Vliet, S Smits, P Rottier, R De Groot]</td>\n",
       "      <td>2002</td>\n",
       "      <td>72afbd0f45b04dc7c0bee89367a3d9813715e675</td>\n",
       "      <td>46.0</td>\n",
       "      <td>[5d21964ac803893c1f15f6a32e740b94380a3199, cc9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  b2897e1277f56641193a6db73825f707eed3e4c9   \n",
       "1  e3d0d482ebd9a8ba81c254cc433f314142e72174   \n",
       "2  00b1d99e70f779eb4ede50059db469c65e8c1469   \n",
       "3  cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0   \n",
       "4  dde02f11923815e6a16a31dd6298c46b109c5dfa   \n",
       "\n",
       "                                               title  \\\n",
       "0  Sequence requirements for RNA strand transfer ...   \n",
       "1  Crystal structure of murine sCEACAM1a[1,4]: a ...   \n",
       "2  Synthesis of a novel hepatitis C virus protein...   \n",
       "3  Structure of coronavirus main proteinase revea...   \n",
       "4  Discontinuous and non-discontinuous subgenomic...   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Nidovirus subgenomic mRNAs contain a leader se...   2001-12-17   \n",
       "1  CEACAM1 is a member of the carcinoembryonic an...   2002-05-01   \n",
       "2  Hepatitis C virus (HCV) is an important human ...   2001-07-16   \n",
       "3  The key enzyme in coronavirus polyprotein proc...   2002-07-01   \n",
       "4  Arteri-, corona-, toro- and roniviruses are ev...   2002-12-01   \n",
       "\n",
       "                                             authors  year  \\\n",
       "0  [Alexander Pasternak, Erwin Van Den Born, Will...  2001   \n",
       "1  [Kemin Tan, Bruce Zelus, Rob Meijers, Jin-Huan...  2002   \n",
       "2  [Zhenming Xu, Jinah Choi, T Yen, Wen Lu, Anne ...  2001   \n",
       "3  [Kanchan Anand, Gottfried Palm, Jeroen Mesters...  2002   \n",
       "4      [A Van Vliet, S Smits, P Rottier, R De Groot]  2002   \n",
       "\n",
       "                                      refid  nbQuotations  \\\n",
       "0  7890bdcde2bc48da8b35296ee38c3aa6e6a549c5          79.0   \n",
       "1  9555d93a1e7d86c279a9cb7f40e1935ac998cb00          54.0   \n",
       "2  592f6ad0e68ffa8c56a5cd5d2eb673bf092b02b3           7.0   \n",
       "3  6df07660b30c131c1d75c1a5974b75faada1044f          20.0   \n",
       "4  72afbd0f45b04dc7c0bee89367a3d9813715e675          46.0   \n",
       "\n",
       "                                          references  \n",
       "0  [63f53cf95376af6f781ae6c60df4887012432de5, 02c...  \n",
       "1  [10e50d9f52a7b77e7d89559892a5898a5a202feb, 736...  \n",
       "2  [11e6080d95677abd1aa156897750ce06c626df43, c35...  \n",
       "3  [65886aa952fa05a113ca27ed8334a3adad3a4aa0, a9a...  \n",
       "4  [5d21964ac803893c1f15f6a32e740b94380a3199, cc9...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. A few stats regarding number of papers loaded\n",
    "\n",
    "print(\"Number of Papers in the CORD-19 dataset :\",dfPaperList.shape[0])\n",
    "#(05/14/2020) Number of Papers in the covid dataset : 63,571\n",
    "\n",
    "print(\"Number of Citations found in the CORD-19 dataset :\",dfCitationsFlat.shape[0])\n",
    "#(05/14/2020) Number of Citations made in the covid dataset : 4,208,974\n",
    "\n",
    "print(\"Citations with no title: \",sum(1 if x == \"\" else 0 for x in dfCitationsFlat[\"title\"]))\n",
    "#(05/14/2020) Citations with no title:  0\n",
    "\n",
    "#How many duplicates? \n",
    "print(\"Number of duplicated research paper titles: \",len(dfPaperList[\"title\"])-len(dfPaperList[\"title\"].drop_duplicates()))\n",
    "#(05/14/2020) Number of duplicated research paper titles:  1,421\n",
    "\n",
    "print(\"Number of duplicated citations titles: \",len(dfCitationsFlat[\"title\"])-len(dfCitationsFlat[\"title\"].drop_duplicates()))\n",
    "#(05/14/2020) Number of duplicated citations titles:  2,543,820\n",
    "\n",
    "#Dataframe Visualization\n",
    "print(\"Number of Papers that will be scored: \", datasetForScoring.shape[0])\n",
    "datasetForScoring.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed283e7",
   "metadata": {},
   "source": [
    "# 2 Computation of Author Scoring and Publication pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1edf9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1. Creating an author dataset + Computation of the author page rank using an author network'''\n",
    "\n",
    "#Variables for author dataset: id, name, co-authors, number of points linked to quotations, paper_count, citations, average citations,co_author_avg_citations,h-index\n",
    "\n",
    "author_data = {}\n",
    "author_id = {\n",
    "    'start': 1,\n",
    "    'curr': 1\n",
    "}\n",
    "\n",
    "assigned_ids = {}\n",
    "\n",
    "def create_author_data(train_data, author_data, author_id, assigned_ids):\n",
    "    for i in range(len(train_data)):\n",
    "        authors = train_data.authors[i]\n",
    "        #This code is accessing the i-th element of the nbQuotations list of the train_data object.\n",
    "    \n",
    "        try:\n",
    "            citations = train_data.nbQuotations[i]/len(authors) #Number of times a paper have been quoted divided by len authors\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        for author in authors:\n",
    "            names = author.split(' ')\n",
    "            unique_name = names[0] + \"_\" + names[len(names)-1]\n",
    "            if unique_name not in author_data:\n",
    "                author_data[unique_name] = {\n",
    "                    'num_citations': citations,\n",
    "                    'paper_count': 1,\n",
    "                    'name': unique_name,\n",
    "                    'author_id': author_id['curr'],\n",
    "                    'co_authors': {},\n",
    "                    'citations': [train_data.nbQuotations[i]]\n",
    "                }\n",
    "                assigned_ids[unique_name] = author_id['curr']\n",
    "                author_id['curr'] += 1\n",
    "\n",
    "            else:\n",
    "                author_data[unique_name]['num_citations'] += citations\n",
    "                author_data[unique_name]['paper_count'] += 1\n",
    "                author_data[unique_name]['citations'].append(train_data.nbQuotations[i])\n",
    "\n",
    "            for co_author in authors:\n",
    "                co_author_names = co_author.split(' ')\n",
    "                co_author_unique_name = co_author_names[0] + \"_\" + co_author_names[len(co_author_names)-1]\n",
    "                if co_author_unique_name != unique_name:\n",
    "                    author_data[unique_name]['co_authors'][co_author_unique_name] = 1\n",
    "                        \n",
    "            \n",
    "            \n",
    "# call for each data file\n",
    "create_author_data(datasetForScoring, author_data, author_id, assigned_ids)\n",
    "\n",
    "# add average citations\n",
    "for data in author_data:\n",
    "    author_data[data]['average_citations'] = author_data[data]['num_citations'] / author_data[data]['paper_count']\n",
    "    \n",
    "# adding h-index\n",
    "def get_h_index(citations):\n",
    "    return ([0] + [i + 1 for i, c in enumerate(sorted(citations, reverse = True)) if c >= i + 1])[-1]\n",
    "\n",
    "data_to_df = []\n",
    "for data in author_data:\n",
    "    each_author = author_data[data]\n",
    "    co_authors = each_author['co_authors']\n",
    "    co_author_ids = []\n",
    "    co_author_avg_citations = 0\n",
    "    for co_author in co_authors:\n",
    "        co_author_avg_citations += author_data[co_author]['average_citations']\n",
    "        co_author_ids.append(assigned_ids[co_author])\n",
    "    each_author['co_authors'] = co_author_ids\n",
    "    each_author['co_author_avg_citations'] = co_author_avg_citations/len(co_author_ids) if len(co_author_ids) != 0 else 0\n",
    "    data_to_df.append(each_author)\n",
    "    \n",
    "authorsData = pd.DataFrame.from_dict(data_to_df, orient='columns')\n",
    "#h_index\n",
    "authorsData['h_index'] = authorsData.apply(lambda x: get_h_index(x.citations), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9aded",
   "metadata": {},
   "source": [
    "### 2. Computation of authors page rank\n",
    "\n",
    "the explanation of How can I convert a Pandas DataFrame to a Python dictionary using the to_dict method: https://blog.gitnux.com/code/pandas-to_dict/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11c7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171295, 5)\n",
      "171295\n"
     ]
    }
   ],
   "source": [
    "'''AUTHOR PAGE RANK'''\n",
    "\n",
    "#Data Pre-processing: building the dataset on which the author network will be built\n",
    "train = authorsData.copy().drop(columns=['num_citations', 'h_index','paper_count', 'citations']).dropna(axis = 0, subset=['co_authors'])\n",
    "train = train[train.co_authors != '[]']\n",
    "train['author_id'] = pd.to_numeric(train['author_id'])\n",
    "#print(train.head(10))\n",
    "print(train.shape)\n",
    "print(len(train))\n",
    "\n",
    "# Building up the network to compute author page rank: \n",
    "G = nx.Graph()\n",
    "for i in range(len(train)):\n",
    "    #select a specific row and column from a Pandas DataFrame1.\n",
    "    auth = train.iloc[i]['author_id'] #select the row at index i\n",
    "    G.add_node(auth)\n",
    "    for neighbor in train.iloc[i]['co_authors']:\n",
    "        if G.has_edge(auth, neighbor):\n",
    "            G.add_edge(auth, neighbor, weight = G[auth][neighbor]['weight']+1)\n",
    "        else:\n",
    "            G.add_edge(auth, neighbor, weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2162bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171295\n",
      "171295\n"
     ]
    }
   ],
   "source": [
    "'''form the edge list'''\n",
    "from collections import defaultdict\n",
    "\n",
    "edge_list=list(G.edges)\n",
    "edges=defaultdict(list)\n",
    "for edge in edge_list:\n",
    "    _from,_to=edge[0],edge[1]\n",
    "    edges[_from].append(_to)\n",
    "    \n",
    "Node_num=len(list(G.nodes))\n",
    "print(max(list(G.nodes)))\n",
    "print(len(list(G.nodes)))\n",
    "#print(list(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc2c7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_authors = nx.pagerank(G, alpha=0.55, max_iter=100, tol=1.0e-6, nstart=None, weight='weight', dangling=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc671cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0\n",
      "index          \n",
      "1      0.000003\n",
      "2      0.000005\n",
      "3      0.000019\n",
      "4      0.000041\n",
      "5      0.000005\n",
      "6      0.000005\n",
      "7      0.000005\n",
      "8      0.000005\n",
      "9      0.000005\n",
      "10     0.000005\n"
     ]
    }
   ],
   "source": [
    "authorPRK = pd.DataFrame.from_dict(score_authors, orient = \"index\").reset_index(drop=False)\n",
    "# The orient parameter is set to \"index\" to indicate that the keys of the dictionary should be used as the row labels of the DataFrame.\n",
    "#print(authorPRK.head(10))\n",
    "#num_rows=authorPRK.shape\n",
    "#print(num_rows)\n",
    "\n",
    "#plt.hist(R,bins=100,density=1)\n",
    "authorPRK=authorPRK.sort_values(by='index')\n",
    "authorPRK.set_index('index',inplace=True)\n",
    "print(authorPRK.head(10))\n",
    "\n",
    "data = authorPRK.values.tolist()\n",
    "Author_pagerank_vector=np.array(data).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e768609",
   "metadata": {},
   "source": [
    "### Trustrank method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6400b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "#from graphs import plotGraph\n",
    "from scipy.sparse import csr_matrix as SparseMatrix\n",
    "\n",
    "class TrustRank:\n",
    "    def __init__(self, beta, edges, epsilon, max_iterations, node_num,PageRank_vector):\n",
    "        self.beta = beta\n",
    "        self.edges = edges\n",
    "        self.epsilon = epsilon\n",
    "        self.node_num = node_num\n",
    "        self.PageRank_vector = PageRank_vector\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "\n",
    "\n",
    "    def get_trustedPages(self, node_number_threshold=100):\n",
    "\n",
    "        # set number of trusted pages\n",
    "        if self.node_num < node_number_threshold:\n",
    "            ratio = 0.2\n",
    "        else:\n",
    "            ratio = 0.0002\n",
    "        trusted_set_size = int(math.ceil(self.node_num * ratio))\n",
    "            ## In Python, math.ceil() is a function that rounds up a given number to the smallest integer greater than or equal to that number\n",
    "\n",
    "\n",
    "        # set and return trusted pages\n",
    "        heaped_ranks = [(rank, node) for (node, rank) in \n",
    "            enumerate(self.PageRank_vector)]\n",
    "            ##  function returns an iterator that generates tuples containing (index, element) pairs2.\n",
    "        heapq._heapify_max(heaped_ranks)\n",
    "        trusted_pages = [heapq._heappop_max(heaped_ranks)[1] \n",
    "            for _ in range(trusted_set_size)]\n",
    "            ## _heappop_max function is used to pop the largest element from the heap and append it to the trusted_pages list for trusted_set_size times1.\n",
    "\n",
    "        return trusted_pages\n",
    "\n",
    "    def get_topicSpecificRank(self, teleport_set):\n",
    "\n",
    "\n",
    "        diff = math.inf\n",
    "        iterations = 0\n",
    "        teleport_set_size = len(teleport_set)\n",
    "\n",
    "        #pg = plotGraph(self.edges, interval=3000)#Time in milli-seconds for which graph is shown on screen\n",
    "        ##jump action define\n",
    "        final_rank_vector = np.zeros(self.node_num)\n",
    "        initial_rank_vector = np.fromiter([1/teleport_set_size if node in teleport_set else 0 for node in\n",
    "                range(self.node_num)], dtype='float')\n",
    "\n",
    "        while(iterations < self.MAX_ITERATIONS and diff > self.epsilon):\n",
    "            new_rank_vector = np.zeros(self.node_num)\n",
    "            for parent in self.edges:\n",
    "                for child in self.edges[parent]:\n",
    "                    \n",
    "                    new_rank_vector[child-1] += self.beta*(initial_rank_vector[parent-1] /len(self.edges[parent]))\n",
    "\n",
    "            leaked_rank = (1 - sum(new_rank_vector)) / teleport_set_size\n",
    "            leaked_rank_vector = np.array([leaked_rank if node in teleport_set\n",
    "                else 0 for node in range(self.node_num)])\n",
    "\n",
    "            final_rank_vector = new_rank_vector + leaked_rank_vector\n",
    "            diff = sum(abs(final_rank_vector - initial_rank_vector))\n",
    "            initial_rank_vector = final_rank_vector\n",
    "\n",
    "            iterations += 1\n",
    "            print(\"TrustRank iteration: \" + str(iterations),\"eps is: \",diff)\n",
    "            #print(final_rank_vector)\n",
    "            #pg.plot(9, final_rank_vector)\n",
    "\n",
    "        return final_rank_vector\n",
    "\n",
    "    def trustRank(self):\n",
    "\n",
    "        trusted_pages = self.get_trustedPages()\n",
    "        print(\"got seed set...\")\n",
    "        final_rank_vector = self.get_topicSpecificRank(trusted_pages)\n",
    "            ##teleport set is a set of pages which are related to each other and belong to same topic.\n",
    "        return final_rank_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b01446c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got seed set...\n",
      "TrustRank iteration: 1  , epsilon: 0.2792616651372623\n",
      "TrustRank iteration: 2  , epsilon: 0.05954281360564107\n",
      "TrustRank iteration: 3  , epsilon: 0.013390212129405124\n",
      "TrustRank iteration: 4  , epsilon: 0.0043142506665514195\n",
      "TrustRank iteration: 5  , epsilon: 0.0014330208742650613\n",
      "TrustRank iteration: 6  , epsilon: 0.0005120461956215936\n",
      "TrustRank iteration: 7  , epsilon: 0.00018921237012757872\n",
      "TrustRank iteration: 8  , epsilon: 7.173473278008748e-05\n",
      "TrustRank iteration: 9  , epsilon: 2.7790886761925418e-05\n",
      "TrustRank iteration: 10  , epsilon: 1.0927547636475961e-05\n",
      "TrustRank iteration: 11  , epsilon: 4.3586148035261325e-06\n",
      "TrustRank iteration: 12  , epsilon: 1.754931661043229e-06\n",
      "TrustRank iteration: 13  , epsilon: 7.142882472935526e-07\n",
      "TrustRank iteration: 14  , epsilon: 2.92845979418456e-07\n",
      "TrustRank iteration: 15  , epsilon: 1.2104992798838934e-07\n",
      "TrustRank iteration: 16  , epsilon: 5.032106040723907e-08\n",
      "TrustRank iteration: 17  , epsilon: 2.1047051772880768e-08\n",
      "TrustRank iteration: 18  , epsilon: 8.845989332420936e-09\n",
      "TrustRank iteration: 19  , epsilon: 3.734750740276542e-09\n",
      "TrustRank iteration: 20  , epsilon: 1.582790576406233e-09\n",
      "TrustRank iteration: 21  , epsilon: 6.730756985566942e-10\n",
      "[4.89075034e-07 2.02390982e-06 3.52705954e-05 ... 2.94727907e-05\n",
      " 2.94727907e-05 2.94727907e-05]\n",
      "1.0000000000001996\n",
      "<built-in method max of numpy.ndarray object at 0x344517a50>\n"
     ]
    }
   ],
   "source": [
    "tr = TrustRank(beta=0.55, edges=edges, epsilon=1e-9, max_iterations=50, node_num=Node_num, PageRank_vector=Author_pagerank_vector)\n",
    "TrustRank_vector = tr.trustRank()\n",
    "print(TrustRank_vector, sum(TrustRank_vector),sep='\\n')\n",
    "print(TrustRank_vector.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb25833",
   "metadata": {},
   "source": [
    "# Get TrustRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efdd35d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171295\n",
      "               0\n",
      "1   6.968438e-07\n",
      "2   2.422135e-06\n",
      "3   4.023194e-05\n",
      "4   4.120967e-05\n",
      "5   3.351893e-06\n",
      "6   5.627968e-06\n",
      "7   1.403218e-05\n",
      "8   1.774342e-05\n",
      "9   1.669777e-05\n",
      "10  6.067036e-07\n",
      "992      0.000573\n",
      "181      0.000340\n",
      "260      0.000296\n",
      "262      0.000261\n",
      "259      0.000218\n",
      "2855     0.000207\n",
      "55487    0.000197\n",
      "4278     0.000189\n",
      "3013     0.000188\n",
      "6449     0.000181\n",
      "Name: 0, dtype: float64\n",
      "index\n",
      "2743     0.000965\n",
      "3346     0.000559\n",
      "2736     0.000313\n",
      "354      0.000289\n",
      "5324     0.000259\n",
      "352      0.000240\n",
      "9158     0.000205\n",
      "17490    0.000202\n",
      "19595    0.000202\n",
      "5469     0.000196\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(len(TrustRank_vector))\n",
    "authorTRK = pd.DataFrame(TrustRank_vector)\n",
    "\n",
    "#print(authorTRK[0].idxmax())\n",
    "authorTRK.index = pd.RangeIndex(start=1, stop=len(authorTRK)+1)\n",
    "#print(authorTRK.head(10))\n",
    "\n",
    "print(authorTRK[0].nlargest(10))\n",
    "print(authorPRK[0].nlargest(10))\n",
    "#print(authorTRK[0].idxmax(10).index)\n",
    "authorTRK[\"author_id\"] = authorTRK.index\n",
    "authorTRK.columns = [\"trustrank_author\", \"author_id\"]\n",
    "#Put page rank value into a csv\n",
    "authorTRK.to_csv(\"trustrank_author.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3458fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorPRK[\"author_id\"] = authorPRK.index\n",
    "authorPRK.columns = [\"pagerank_author\", \"author_id\"]\n",
    "#Put page rank value into a csv\n",
    "authorPRK.to_csv(\"pagerank_author.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1abde",
   "metadata": {},
   "source": [
    "### 3. Computation of publication page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "670f1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Computation of publication page rank'''\n",
    "\n",
    "# Building up the network to compute the pagerank for publication\n",
    "G1 = nx.Graph()\n",
    "for i in range(len(datasetForScoring)):\n",
    "# for i in range(100): #Only on a sample\n",
    "    G1.add_node(datasetForScoring['refid'][i])\n",
    "    auth = datasetForScoring['refid'][i]\n",
    "    \n",
    "    for e in list(str(datasetForScoring[\"references\"][i]).lstrip(\"[\").rstrip(\"]\").replace(\" \",\"\").split(\",\")):\n",
    "        try:\n",
    "            if G1.has_edge(auth, e):\n",
    "                G1.add_edge(auth, e, weight = G[auth][e]['weight']+1)\n",
    "            else:\n",
    "                G1.add_edge(auth, e, weight = 1)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "score_publication = nx.pagerank(G1, alpha=0.85, tol=1.0e-6, nstart=None, weight=1, dangling=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6023d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0038e56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'63f53cf95376af6f781ae6c60df4887012432de5'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'02cc62567a75705831e809af741b9c6a47b2b186'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'b45f834b897ac6b39242087a9f37c58fd333c36f'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'6c0db1a2670ef286cd188498417422dd781aa88e'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'5d21964ac803893c1f15f6a32e740b94380a3199'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'7691d766309a4424cf53095848d799a6ac0729ab'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'cc9e3b01bf4c4c176ed5b53fe5d6d7d6e5f249a3'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'f680512ee0f89db57154ea8867ca53dc29058ab8'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'0f41ee565fbeff1e1535048f886e6f3042ebe96f'\"), ('7890bdcde2bc48da8b35296ee38c3aa6e6a549c5', \"'37a2fb1980e2a45cd9bd8826561c7eafb38276b3'\")]\n",
      "1371165\n",
      "46407\n"
     ]
    }
   ],
   "source": [
    "edge_list1=list(G1.edges)\n",
    "print(edge_list1[:10])\n",
    "edges1=defaultdict(list)\n",
    "for edge in edge_list1:\n",
    "    _from,_to=edge[0],edge[1]\n",
    "    edges1[_from].append(_to)\n",
    "    \n",
    "Node_num=len(list(G1.nodes))\n",
    "print(Node_num)\n",
    "print(len(datasetForScoring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f98412e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        index             0\n",
      "0    7890bdcde2bc48da8b35296ee38c3aa6e6a549c5  2.208890e-06\n",
      "1  '63f53cf95376af6f781ae6c60df4887012432de5'  1.196131e-06\n",
      "2  '02cc62567a75705831e809af741b9c6a47b2b186'  7.544144e-07\n",
      "3  'b45f834b897ac6b39242087a9f37c58fd333c36f'  3.330688e-07\n",
      "4  '6c0db1a2670ef286cd188498417422dd781aa88e'  5.808578e-07\n",
      "5  '5d21964ac803893c1f15f6a32e740b94380a3199'  1.638114e-06\n",
      "6  '7691d766309a4424cf53095848d799a6ac0729ab'  3.414348e-06\n",
      "7  'cc9e3b01bf4c4c176ed5b53fe5d6d7d6e5f249a3'  5.168574e-07\n",
      "8  'f680512ee0f89db57154ea8867ca53dc29058ab8'  1.907804e-06\n",
      "9  '0f41ee565fbeff1e1535048f886e6f3042ebe96f'  3.330688e-07\n",
      "                                         index             0  index1\n",
      "1     7890bdcde2bc48da8b35296ee38c3aa6e6a549c5  2.208890e-06       1\n",
      "2   '63f53cf95376af6f781ae6c60df4887012432de5'  1.196131e-06       2\n",
      "3   '02cc62567a75705831e809af741b9c6a47b2b186'  7.544144e-07       3\n",
      "4   'b45f834b897ac6b39242087a9f37c58fd333c36f'  3.330688e-07       4\n",
      "5   '6c0db1a2670ef286cd188498417422dd781aa88e'  5.808578e-07       5\n",
      "6   '5d21964ac803893c1f15f6a32e740b94380a3199'  1.638114e-06       6\n",
      "7   '7691d766309a4424cf53095848d799a6ac0729ab'  3.414348e-06       7\n",
      "8   'cc9e3b01bf4c4c176ed5b53fe5d6d7d6e5f249a3'  5.168574e-07       8\n",
      "9   'f680512ee0f89db57154ea8867ca53dc29058ab8'  1.907804e-06       9\n",
      "10  '0f41ee565fbeff1e1535048f886e6f3042ebe96f'  3.330688e-07      10\n",
      "                               publication_id  pageRankPublication  index1\n",
      "1    7890bdcde2bc48da8b35296ee38c3aa6e6a549c5         2.208890e-06       1\n",
      "2  '63f53cf95376af6f781ae6c60df4887012432de5'         1.196131e-06       2\n",
      "3  '02cc62567a75705831e809af741b9c6a47b2b186'         7.544144e-07       3\n",
      "4  'b45f834b897ac6b39242087a9f37c58fd333c36f'         3.330688e-07       4\n",
      "5  '6c0db1a2670ef286cd188498417422dd781aa88e'         5.808578e-07       5\n",
      "                               publication_id  index1\n",
      "0    7890bdcde2bc48da8b35296ee38c3aa6e6a549c5       1\n",
      "1  '63f53cf95376af6f781ae6c60df4887012432de5'       2\n",
      "2  '02cc62567a75705831e809af741b9c6a47b2b186'       3\n",
      "3  'b45f834b897ac6b39242087a9f37c58fd333c36f'       4\n",
      "4  '6c0db1a2670ef286cd188498417422dd781aa88e'       5\n"
     ]
    }
   ],
   "source": [
    "#Saving the page rank by paper id\n",
    "publiPRK = pd.DataFrame.from_dict(score_publication, orient = \"index\").reset_index(drop=False)\n",
    "print(publiPRK.head(10))\n",
    "publiPRK.index = pd.RangeIndex(start=1, stop=len(publiPRK)+1)\n",
    "publiPRK[\"index1\"] = publiPRK.index\n",
    "print(publiPRK.head(10))\n",
    "#publiPRK[\"publication_id\"] = publiPRK['index']\n",
    "#print(type(publiPRK[\"publication_id\"]))\n",
    "#publiPRK.columns = [\"pageRankPublication\",\"publication_id\"]\n",
    "publiPRK.columns = [\"publication_id\",\"pageRankPublication\",'index1']\n",
    "cols_forcal=[ \"publication_id\",'index1']\n",
    "print(publiPRK.head())\n",
    "#publiPRK[\"publication_id\"] = publiPRK[\"publication_id\"].str.replace(\"'\",\"\")\n",
    "publiPRK_forcal = publiPRK[cols_forcal].reset_index(drop = True)\n",
    "print(publiPRK_forcal.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a02f898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#authorPRK = pd.DataFrame.from_dict(score_authors, orient = \"index\").reset_index(drop=False)\n",
    "# The orient parameter is set to \"index\" to indicate that the keys of the dictionary should be used as the row labels of the DataFrame.\n",
    "#print(authorPRK.head(10))\n",
    "#num_rows=authorPRK.shape\n",
    "#print(num_rows)\n",
    "\n",
    "#plt.hist(R,bins=100,density=1)\n",
    "data1 = publiPRK_forcal.values.tolist()\n",
    "#print(data1[:100])\n",
    "Public_pagerank_vector=np.array(data1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "264d7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "#from graphs import plotGraph\n",
    "from scipy.sparse import csr_matrix as SparseMatrix\n",
    "\n",
    "class TrustRank:\n",
    "    def __init__(self, beta, edges, epsilon, max_iterations, node_num,PageRank_vector,publiPRK_forcal):\n",
    "        self.beta = beta\n",
    "        self.edges = edges\n",
    "        self.epsilon = epsilon\n",
    "        self.node_num = node_num\n",
    "        self.PageRank_vector = PageRank_vector\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "        self.publiPRK_forcal=publiPRK_forcal\n",
    "\n",
    "\n",
    "    def get_trustedPages(self, node_number_threshold=100):\n",
    "\n",
    "        # set number of trusted pages\n",
    "        if self.node_num < node_number_threshold:\n",
    "            ratio = 0.2\n",
    "        else:\n",
    "            ratio = 0.0002\n",
    "        trusted_set_size = int(math.ceil(self.node_num * ratio))\n",
    "            ## In Python, math.ceil() is a function that rounds up a given number to the smallest integer greater than or equal to that number\n",
    "\n",
    "\n",
    "        # set and return trusted pages\n",
    "        heaped_ranks = [(rank, node) for (node, rank) in \n",
    "            enumerate(self.PageRank_vector)]\n",
    "            ##  function returns an iterator that generates tuples containing (index, element) pairs2.\n",
    "        heapq._heapify_max(heaped_ranks)\n",
    "        trusted_pages = [heapq._heappop_max(heaped_ranks)[1] \n",
    "            for _ in range(trusted_set_size)]\n",
    "            ## _heappop_max function is used to pop the largest element from the heap and append it to the trusted_pages list for trusted_set_size times1.\n",
    "\n",
    "        return trusted_pages\n",
    "\n",
    "    def get_topicSpecificRank(self, teleport_set):\n",
    "\n",
    "\n",
    "        diff = math.inf\n",
    "        iterations = 0\n",
    "        teleport_set_size = len(teleport_set)\n",
    "\n",
    "        #pg = plotGraph(self.edges, interval=3000)#Time in milli-seconds for which graph is shown on screen\n",
    "        ##jump action define\n",
    "        final_rank_vector = np.zeros(self.node_num)\n",
    "        initial_rank_vector = np.fromiter([1/teleport_set_size if node in teleport_set else 0 for node in\n",
    "                range(self.node_num)], dtype='float')\n",
    "\n",
    "        while(iterations < self.MAX_ITERATIONS and diff > self.epsilon):\n",
    "            new_rank_vector = np.zeros(self.node_num)\n",
    "            for parent in self.edges:\n",
    "                parent_value = self.publiPRK_forcal.loc[self.publiPRK_forcal[\"publication_id\"] == parent, \"index1\"].values[0]\n",
    "                for child in self.edges[parent]:\n",
    "                    child_value = self.publiPRK_forcal.loc[self.publiPRK_forcal[\"publication_id\"] == child, \"index1\"].values[0]\n",
    "                    new_rank_vector[child_value-1] += self.beta*(initial_rank_vector[parent_value-1] /len(self.edges[parent]))\n",
    "\n",
    "            leaked_rank = (1 - sum(new_rank_vector)) / teleport_set_size\n",
    "            leaked_rank_vector = np.array([leaked_rank if node in teleport_set\n",
    "                else 0 for node in range(self.node_num)])\n",
    "\n",
    "            final_rank_vector = new_rank_vector + leaked_rank_vector\n",
    "            diff = sum(abs(final_rank_vector - initial_rank_vector))\n",
    "            initial_rank_vector = final_rank_vector\n",
    "\n",
    "            iterations += 1\n",
    "            print(\"TrustRank iteration: \" + str(iterations),\"eps is: \",diff)\n",
    "            #print(final_rank_vector)\n",
    "            #pg.plot(9, final_rank_vector)\n",
    "\n",
    "        return final_rank_vector\n",
    "\n",
    "    def trustRank(self):\n",
    "\n",
    "        trusted_pages = self.get_trustedPages()\n",
    "        print(\"got seed set...\")\n",
    "        final_rank_vector = self.get_topicSpecificRank(trusted_pages)\n",
    "            ##teleport set is a set of pages which are related to each other and belong to same topic.\n",
    "        return final_rank_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got seed set...\n"
     ]
    }
   ],
   "source": [
    "publi = TrustRank(beta=0.85, edges=edges1, epsilon=1e-9, max_iterations=50, node_num=Node_num, PageRank_vector=Public_pagerank_vector,publiPRK_forcal=publiPRK_forcal)\n",
    "TrustRank_vector = publi.trustRank()\n",
    "print(TrustRank_vector, sum(TrustRank_vector),sep='\\n')\n",
    "print(TrustRank_vector.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b50049",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"pageRankPublication\", \"publication_id\"]\n",
    "print(publiPRK.head())\n",
    "publiPRK[\"publication_id\"] = publiPRK[\"publication_id\"].str.replace(\"'\",\"\")\n",
    "publiPRK = publiPRK[cols].reset_index(drop = True)\n",
    "print(publiPRK.head())\n",
    "\n",
    "publiPRK.to_csv(\"pagerank_publication.csv\",index = False)\n",
    "\n",
    "#Integration of the variable Page Rank for publication datasetForScoring\n",
    "enhancedDatasetForScoring = pd.merge(datasetForScoring,publiPRK, left_on = \"refid\", right_on = \"publication_id\", how = \"left\").drop(columns= [\"publication_id\"])\n",
    "enhancedDatasetForScoring = enhancedDatasetForScoring.drop_duplicates(subset='refid', keep=\"last\") #Temporary patch to manage the case where twice Page rank for some publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d741ec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mapped_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpubliPRK_forcal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpubliPRK_forcal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_list1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mapped_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpubliPRK_forcal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpubliPRK_forcal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, edge_list1))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "mapped_list = list(map(lambda x: publiPRK_forcal.loc[publiPRK_forcal['publication_id'] == x, 'index1'].values[0], edge_list1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13ba1722",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m row \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(edge_list1[i])):\n\u001b[0;32m----> 8\u001b[0m     mapped_value \u001b[38;5;241m=\u001b[39m publiPRK_forcal\u001b[38;5;241m.\u001b[39mloc[\u001b[43mpubliPRK_forcal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43medge_list1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m     row\u001b[38;5;241m.\u001b[39mappend(mapped_value)\n\u001b[1;32m     10\u001b[0m mapped_list\u001b[38;5;241m.\u001b[39mappend(row)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/ops/common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:5623\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5620\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 5623\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:283\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:73\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the mapped values\n",
    "mapped_list = []\n",
    "\n",
    "# Use nested loops and map function to apply the mapping to the 2D list\n",
    "for i in range(len(edge_list1)):\n",
    "    row = []\n",
    "    for j in range(len(edge_list1[i])):\n",
    "        mapped_value = publiPRK_forcal.loc[publiPRK_forcal['publication_id'] == edge_list1[i][j], 'index1'].values[0]\n",
    "        row.append(mapped_value)\n",
    "    mapped_list.append(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
